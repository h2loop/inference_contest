{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Optimization Challenge — Qwen2.5-0.5B on Tesla T4\n",
    "\n",
    "**Goal:** Maximize output token throughput (tok/s) for Qwen2.5-0.5B on a Tesla T4 GPU.  \n",
    "**Baseline to beat:** 3,332 tok/s\n",
    "\n",
    "## Scoring\n",
    "\n",
    "| Metric | Weight | Constraint |\n",
    "|--------|--------|------------|\n",
    "| Output throughput (tok/s) | 40% | Higher is better |\n",
    "| P99 TPOT | 20% | Must stay under 50 ms |\n",
    "| P99 TTFT | 15% | Must stay under 2000 ms |\n",
    "| Request success rate | 10% | Must be 100% |\n",
    "| Code quality & documentation | 15% | Clean, reproducible, explained |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Launch vLLM Server (Baseline — Default Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests\n",
    "\n",
    "MODEL = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "# Launch vLLM server in the background\n",
    "server_proc = subprocess.Popen(\n",
    "    [\"vllm\", \"serve\", MODEL],\n",
    "    stdout=open(\"server_baseline.log\", \"w\"),\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "print(f\"Server PID: {server_proc.pid}\")\n",
    "\n",
    "# Wait for server to be ready\n",
    "for i in range(120):\n",
    "    try:\n",
    "        r = requests.get(\"http://localhost:8000/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"Server ready after {i*2}s\")\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(2)\n",
    "else:\n",
    "    print(\"ERROR: Server not ready after 240s\")\n",
    "    print(open(\"server_baseline.log\").read()[-2000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Benchmark (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p results\n",
    "\n",
    "!vllm bench serve \\\n",
    "  --backend openai \\\n",
    "  --base-url http://localhost:8000/v1 \\\n",
    "  --endpoint /completions \\\n",
    "  --model Qwen/Qwen2.5-0.5B \\\n",
    "  --tokenizer Qwen/Qwen2.5-0.5B \\\n",
    "  --max-concurrency 50 \\\n",
    "  --num-prompts 200 \\\n",
    "  --ignore-eos \\\n",
    "  --random-input-len 512 \\\n",
    "  --random-output-len 512 \\\n",
    "  --save-result \\\n",
    "  --result-dir ./results \\\n",
    "  --result-filename baseline.json \\\n",
    "  --label baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Baseline Results\n\n```\n============ Serving Benchmark Result ============\nSuccessful requests:                     200       \nFailed requests:                         0         \nMaximum request concurrency:             50        \nBenchmark duration (s):                  29.43     \nTotal input tokens:                      102400    \nTotal generated tokens:                  102400    \nRequest throughput (req/s):              6.80      \nOutput token throughput (tok/s):         3479.29   \nPeak output token throughput (tok/s):    4248.00   \nPeak concurrent requests:                88.00     \nTotal token throughput (tok/s):          6958.58   \n---------------Time to First Token----------------\nMean TTFT (ms):                          396.79    \nMedian TTFT (ms):                        313.92    \nP99 TTFT (ms):                           1161.99   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          13.59     \nMedian TPOT (ms):                        13.71     \nP99 TPOT (ms):                           14.12     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           13.59     \nMedian ITL (ms):                         12.44     \nP99 ITL (ms):                            79.73     \n==================================================\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\nwith open(\"results/baseline.json\") as f:\n    data = json.load(f)\n\ntotal = data['completed'] + data.get('failed', 0)\n\nprint(\"=\" * 60)\nprint(\"BASELINE RESULTS\")\nprint(\"=\" * 60)\nprint(f\"  Output throughput:  {data['output_throughput']:.2f} tok/s\")\nprint(f\"  Mean TPOT:          {data['mean_tpot_ms']:.2f} ms\")\nprint(f\"  P99 TPOT:           {data['p99_tpot_ms']:.2f} ms  (limit: 50 ms)\")\nprint(f\"  Mean TTFT:          {data['mean_ttft_ms']:.2f} ms\")\nprint(f\"  P99 TTFT:           {data['p99_ttft_ms']:.2f} ms  (limit: 2000 ms)\")\nprint(f\"  Completed requests: {data['completed']}/{total}\")\nprint(f\"  Failed requests:    {data.get('failed', 0)}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_proc.terminate()\n",
    "server_proc.wait()\n",
    "print(\"Server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}